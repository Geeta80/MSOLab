{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YjCvocbEfr4v"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iF-osqNxgy4y"
   },
   "outputs": [],
   "source": [
    "def build_graph_Q(state, action):\n",
    "    with tf.variable_scope('layer0'):\n",
    "        layer_size = 128\n",
    "        state0 = tf.layers.dense(state,layer_size)\n",
    "        action0 = tf.layers.dense(action, layer_size, use_bias=False) \n",
    "        layer = action0 + state0 # layer này là 1 Affine Transformation cúa state và action Wt*s + Wt*a + b\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    with tf.variable_scope('layer1'):\n",
    "        layer_size = 128\n",
    "        layer = tf.layers.dense(layer, layer_size)\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "    with tf.variable_scope('layer2'):\n",
    "        layer_size = 1\n",
    "        layer = tf.layers.dense(layer, layer_size)\n",
    "\n",
    "    return layer\n",
    "\n",
    "def build_graph_policy(state):\n",
    "    with tf.variable_scope('layer0'):\n",
    "        layer_size = 128\n",
    "        layer = tf.layers.dense(state, layer_size)\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    with tf.variable_scope('layer1'):\n",
    "        layer_size = 128\n",
    "        layer = tf.layers.dense(layer, layer_size)\n",
    "        layer = tf.nn.relu(layer)\n",
    "        \n",
    "    with tf.variable_scope('layer2'):\n",
    "        layer_size = 1\n",
    "        layer = tf.layers.dense(layer, layer_size)\n",
    "        layer = tf.nn.tanh(layer)\n",
    "        layer = tf.multiply(layer, 2) #action space từ -2 đến 2 nên ta dùng activation function tanh() sau đó nhân với 2\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sWqGWvRnfu3R"
   },
   "outputs": [],
   "source": [
    "observations_ph = tf.placeholder(tf.float32, shape=(None, 3), name='observation')\n",
    "next_observations_ph = tf.placeholder(tf.float32, shape=(None, 3), name='next_observation')\n",
    "rewards_ph = tf.placeholder(tf.float32, shape=(None, 1), name='reward')\n",
    "actions_ph = tf.placeholder(tf.float32, shape=(None, 1), name='action')\n",
    "terminals_ph = tf.placeholder(tf.float32, shape=(None, 1), name='terminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZeOzLFV4fvNn",
    "outputId": "7a109a42-b0a9-44a6-d153-0c51a8bcb3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ba949f59ecbf>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/grey/anaconda3/lib/python3.8/site-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/grey/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('Actor/eval'):\n",
    "    policy = build_graph_policy(observations_ph)\n",
    "with tf.variable_scope('Actor/target'):\n",
    "    target_policy = build_graph_policy(observations_ph)\n",
    "\n",
    "with tf.variable_scope('Critic/eval'):\n",
    "    q = build_graph_Q(observations_ph, actions_ph) \n",
    "with tf.variable_scope('Critic/eval', reuse=True):\n",
    "    q_policy = build_graph_Q(observations_ph, policy)\n",
    "\n",
    "with tf.variable_scope('Critic/target'):\n",
    "    target_q = build_graph_Q(next_observations_ph, target_policy)\n",
    "\n",
    "policy_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "target_policy_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "q_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "target_q_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jdAfvkoyf9jR"
   },
   "outputs": [],
   "source": [
    "TAU = 0.01\n",
    "GAMMA = 0.9\n",
    "LR_Critic = 0.001\n",
    "LR_Actor = 0.0001\n",
    "\n",
    "q_target = tf.stop_gradient(rewards_ph + (1-terminals_ph)*GAMMA*target_q)\n",
    "\n",
    "q_error = 0.5*tf.reduce_mean((q_target - q)**2)\n",
    "q_train_ops = tf.train.AdamOptimizer(LR_Critic).minimize(loss=q_error, var_list=q_params)\n",
    "\n",
    "policy_loss = - tf.reduce_mean(q_policy)\n",
    "policy_train_ops = tf.train.AdamOptimizer(LR_Actor).minimize(loss=policy_loss, var_list=policy_params)\n",
    "\n",
    "update_policy_ops = [tf.assign(tpp, (1-TAU)*tpp + TAU*pp) for tpp, pp in zip(target_policy_params, policy_params)]\n",
    "update_q_ops = [tf.assign(tqp, (1-TAU)*tqp + TAU*qp) for tqp, qp in zip(target_q_params, q_params)]\n",
    "\n",
    "def action_respond(sess, obs):\n",
    "    action = sess.run(policy, feed_dict={observations_ph: obs})[0]\n",
    "    return action\n",
    "\n",
    "def init_training(sess):\n",
    "    sess.run(update_policy_ops)\n",
    "    sess.run(update_q_ops)\n",
    "\n",
    "def get_feed_dict(batch):\n",
    "    feed_dict = {observations_ph: batch['observations'],\n",
    "                 actions_ph: batch['actions'],\n",
    "                 next_observations_ph: batch['next_observations'],\n",
    "                 rewards_ph: batch['rewards'],\n",
    "                 terminals_ph: batch['terminals']}\n",
    "    return feed_dict\n",
    "    \n",
    "def do_training(sess, batch):\n",
    "    feed_dict = get_feed_dict(batch)\n",
    "    sess.run([q_train_ops, policy_train_ops], feed_dict)\n",
    "    sess.run(update_policy_ops)\n",
    "    sess.run(update_q_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZbIrCbFQhBcK"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0') # tạo một environment\n",
    "env.reset() #  reset lại environment, hàm reset() trả về state đầu tiên của environment\n",
    "episodes = 10 # ta chạy 10 episodes\n",
    "steps = 1000 # mỗi episodes ta chạy nhiều nhất 100 steps\n",
    "for ep in range(episodes):\n",
    "  state = env.reset()\n",
    "  for step in range(steps):\n",
    "    # env.render() # gọi hàm render() để sinh ra animation, mình hay tắt đi vì nó gây crash trên window\n",
    "    action = env.action_space.sample() # lấy 1 action ngẫu nhiên trong action space\n",
    "    next_state, reward, done, _ = env.step(action) # thực thi action đó trên environment, giá trị trả về là state tiếp theo s', reward nhận được, và done (đã kết thúc episodes hay chưa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DCvwWnqdhTtO"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0') \n",
    "env.reset() \n",
    "episodes = 10 \n",
    "steps = 1000 \n",
    "\n",
    "BUFFER_SIZE = 1000000 # độ lớn của buffer\n",
    "buffer = []\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    for step in range(steps):\n",
    "        # env.render()\n",
    "        action = env.action_space.sample() \n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        \n",
    "        if len(buffer) >= BUFFER_SIZE: \n",
    "             buffer.pop(0)  # nếu buffer đầy thì pop phần tử đầu tiên ra      \n",
    "        buffer.append([state, action, np.array([reward]), np.array([done]).astype(int), next_state]) #thêm experience mới\n",
    "        state = next_state #sau khi lưu vào buffer xong thì ta gán state là next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1J4Q0z8hyU2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ddpg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
